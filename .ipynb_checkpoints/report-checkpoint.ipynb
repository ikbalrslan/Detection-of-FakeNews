{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>BBM 409: Introduction to Machine Learning Lab.</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 2</center></h1>\n",
    "<h1><center>Due on November 24, 2018 (23:59:59)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Muhammed İkbal Arslan</h3> \n",
    "<h3 align=\"center\">21426611</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-)** If we derive the MLE estimator for the mean $\\mu$:\n",
    "$$P(x1, . . . , xN |\\mu) = \\displaystyle\\prod_{i=1}^{N} P(X_{i}|\\mu) $$\n",
    "\n",
    "we can maximize the likelihood with log and we can take derivatives of this with respect to $\\mu$ and find:\n",
    "\n",
    "$$log(P(x1, . . . , xN |\\mu)) = \\displaystyle\\sum_{i=1}^{N} log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma ^ 2}} \\right) - \\frac{{(x_{i} - \\mu)}^2}{2 \\sigma ^ 2} \\implies \\frac{dlog(P(x1, . . . , xN |\\mu))}{d\\mu} = \\displaystyle\\sum_{i=1}^{N} \\frac{(x_{i} - \\mu)}{\\sigma ^ 2} $$\n",
    "\n",
    "After that we should apply the left hand side rule on equation above for eliminate the negative summation of $\\displaystyle\\sum_{i=1}^{N} \\mu $ , we can find $\\hat\\mu$ value as below:\n",
    "\n",
    "$$0 = \\displaystyle\\sum_{i=1}^{N}(x_{i} - \\mu) \\implies N\\mu = \\displaystyle\\sum_{i=1}^{N}x_{i} \\implies \\hat\\mu = \\frac{\\displaystyle\\sum_{i=1}^{N}x_{i}}{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-)** Conditional independence assumption the probability of observing a vector x can be below based on the Naive Bayes: \n",
    "$$ p(xi = 0|c) = 1 − p(xi = 1|c) = 1 − \\theta^c_i.$$\n",
    "$$ p(x|c) = \\displaystyle\\prod_{i=1}^{D} p(x_i|c) = \\displaystyle\\prod_{i=1}^{D}(\\theta_i^c)^{x_i} (1 - \\theta_i^c)^{1 - x_i}$$\n",
    "\n",
    "If we look at the above expression, $x_i$ can be 0 or 1. Together with the assumption that the training data is  generated, the log likelihood of the attributes and class labels is:\n",
    "\n",
    "$$ L = \\displaystyle\\sum_{n} log\\space p(x^n,c^n) = \\displaystyle\\sum_{n} log\\space p(c^n) \\displaystyle\\prod_{i} p(x_i^n|c^n) $$\n",
    "\n",
    "$$\\implies \\displaystyle\\sum_{i,n} x_i^n log\\theta_i^{c^n} + (1 - x_i^n) log (1-\\theta_i^{c^n} + n_0 log p(c=0) + n_1 log p(c=1))$$\n",
    "\n",
    "We can find the Maximum Likelihood optimal θ_i^c by differentiating by θ_i^c and equating to zero:\n",
    "$$\\theta_i^c = p(x_i = 1|c) = \\frac{frequency\\space of\\space x_i = 1\\space of\\space class\\space c}{count\\space of\\space dataset\\space in\\space class\\space c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-)** The sample is (3,0,2,1,3,2,1,0,2,1),so the likelihood *L($\\theta$)* is: \n",
    "\n",
    "$$ L(θ) = P(X = 3) \\times P(X = 0) \\times P(X = 2) \\times P(X = 1) \\times P(X = 3) \\times $$ $$ P(X = 2) \\times P(X = 1) \\times P(X = 0) \\times P(X = 2) \\times P(X = 1)$$\n",
    "\n",
    "If we use probability distribution given above, we reach the following formula below:\n",
    "\n",
    "$$L(\\theta) = \\displaystyle\\prod_{i=1}^{n} P(X_{i}|θ) = \\left(\\frac{2\\theta}{3}\\right)^2    \\left(\\frac{\\theta}{3}\\right)^3   \\left(\\frac{2(1-\\theta)}{3}\\right)^3  \\left(\\frac{(1-\\theta)}{3}\\right)^2$$\n",
    "\n",
    "After that we should look at the log likelihood function. Because, we can see that the log likelihood function is easier than the likelihood function about their maximize compared.\n",
    "\n",
    "$$ log\\space L(\\theta) = \\displaystyle\\sum_{i=1}^{n} log \\space P(X_{i}|\\theta) \\implies 2 \\left( log \\frac{2}{3} + log \\theta \\right) + 3 \\left( log \\frac{1}{3} + log \\theta \\right) + 3 \\left( log \\frac{2}{3} + log (1-\\theta) \\right) + 2 \\left( log \\frac{1}{3} + log (1-\\theta) \\right) $$\n",
    "\n",
    "$$ \\implies -3.26606256888 + 5log\\theta + 5log(1-\\theta) $$\n",
    "\n",
    "-3.26606256888 is a constant which does not effect or depend on $\\theta$. While log $L(\\theta$) = $l(\\theta$), if we take derivative of l($\\theta$) with respect to $\\theta$:\n",
    "\n",
    "$$\\frac{dl(\\theta)}{d\\theta} = \\frac{5}{\\theta} - \\frac{5}{1-\\theta} = 0$$\n",
    "- We find the **maximum likelihood estimate of θ as 0.5** from result of calculation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table is created for the following responses were obtained from people who claimed also to be ’content’: (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1) and for ’not content’: (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Document | Word1|  Word2  | Word3   |Class|\n",
    "|----------|------|---------|---------|-----|\n",
    "|     1    | rich | married | healty  | content|\n",
    "|     2    | unrich | unmarried | healty  | content|\n",
    "|     3    | rich | married | unhealty  | content|\n",
    "|     4    | rich | unmarried | healty  | content|\n",
    "|     5    | unrich | unmarried | unhealty  | not content|\n",
    "|     6    | rich | unmarried | unhealty  | not content|\n",
    "|     7    | unrich | unmarried | healty  | not content|\n",
    "|     8    | unrich | married | unhealty  | not content|\n",
    "|     9    | unrich | unmarried | unhealty  | not content|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-) Firstly we compute prior probabilities of classes:**\n",
    "$$ P(content) = \\frac{4}{9}\\space and \\space P(not\\space content) = \\frac{5}{9}$$\n",
    "\n",
    "For (0,1,1) $\\implies$ unrich, married, healty; **I applied laplace smoothing too:**\n",
    "$$ P(unrich | content) = \\frac{1+1}{12+6} = \\frac{2}{18} = \\frac{1}{9} \\space,\\space  P(married | content) = \\frac{2+1}{12+6} = \\frac{3}{18} = \\frac{1}{6} \\space , \\space P(healty | content) = \\frac{3+1}{12+6} = \\frac{4}{18} = \\frac{2}{9} $$\n",
    "\n",
    "$$P((0,1,1) | content) = \\frac{4}{9}*\\frac{1}{9}*\\frac{1}{6}*\\frac{2}{9} = 0.001828989$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-)** For (0,1,x) $\\implies$ unrich, married, healty or unhealty; **I applied laplace smoothing too:** \n",
    "- Steps are the same with the previous solution (1), we know the probabilies of $ P(unrich | content) \\space ,\\space  P(married | content) \\space and \\space P(healty | content) $\n",
    "\n",
    "Just we should compute the P(unhealty|content) probability additionally.\n",
    "$$ P(unhealty | content) = \\frac{1+1}{12+6} = \\frac{2}{18} = \\frac{1}{9}$$\n",
    "\n",
    "$$P((0,1,x) | content) = \\frac{4}{9}*\\frac{1}{9}*\\frac{1}{6}* \\left( \\frac{2}{9} + \\frac{1}{9} \\right) = 0.002743484$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In determining whether the news headlines are real or fake news, I will try to explain my steps in this report. In the first part, I will talk about the process of reading the train and test files and talk about the stem and tokenizing processes that I have followed to increase the accuracy calculation for the model and make the program more accurate. The second part is about understanding data topic. I will try to determine 3 examples of specific keywords that may be useful for classifiying, together with statistics on how often they appear in real and fake headlines. In the third part, I will talk about my steps and how I wrote Naive Bayes classifier implementation on Unigram and Bigram options. Unigram is the occurrences of words in a document and Bigram is the occurrences of two adjacent words in a document. Additionally, I will some hint about The bag-of-words model in here. (This is a way of representing text data when modeling text with machine learning algorithms.) In the next step, by analyzing the effects of words on predicting, I will explain the reasons by observing the change in my results by applying the stopwords process. Finally, I will give the accuracy of the calculation results and explain separately how the processes leads to a result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readFile import readFile\n",
    "from bagOfWords import bagOfWords,tenWordsWithCondProb, tenWordsWith_Tf_Idf\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from readFile import readCSV\n",
    "from naiveBayes import naiveBayes,calculationofAccuracy,understandData,findTenNonStopWords\n",
    "import numpy as np\n",
    "from math import log10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries used for creating numpy arrays, parsing .csv files into the dataframes, parsing train .txt files, stem operations and discarding to the stopwords. Additionally, vectorizer operations was done for NLP and Machine Learning problems, such as bag of words, transform and indexing words into dictinaries. But mostly, my approaches was mostly effected the solution. i.e. for example CountVectorizer can lowercase letters, disregard punctuation and stopwords, but it can't stem. Because of that, I made stem operations with my approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**  *Read given real and fake dataset and split them according to needs of operations. (options such as stem, stopword, tokenizing, etc.)*<br>\n",
    "**--**  *After that, read given test .csv file and do the same operations on it for unigram and bigram words coupling*<br>\n",
    "**--**  *Prepare dataset for fake/real classification.*<br>\n",
    "**--**  *Filter data for different options, make stopword, stem and tokenizing on it*<br>\n",
    "**--**  *Understand the data with your own metodology.*<br>\n",
    "**--**  *Imlement Bag of Words (BoW) model.*<br>\n",
    "**--**  *Imlement Naive Bayes Algorithm for Unigram and Bigram.*<br>\n",
    "**--**  *Compute the log probabilities and make laplace smoothing on it.*<br>\n",
    "**--**  *Analyze the effect of the words on prediction. Give some approaches.*<br>\n",
    "**--**  *Calculate the accuracy of the classification which according to the given formula.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Writing Purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- readFile(trainReal, trainFake, ngram, stemed): reads trainReal and trainFake for different ngram and stemed parameters\n",
    "- readCSV(testFile, ngram, stemed): reads test.csv for different ngram and stemed parameters\n",
    "- bagOfWords(linesOfReal,linesOfFake, ngram, stopEnglish, stemed): takes option parameters, calls naive bayes and understandData functions for processes\n",
    "- naiveBayes(countOfRealsDict, BoWOfReal, countOfFakesDict, BoWOfFake, testindexDictOfWord, test_BoW): takes some count parameters and BoW of datas, after that performs naive bayes algorithm \n",
    "- calculationofAccuracy(correctnessCount, testSize): for accuracy calculation\n",
    "- understandData(countOfRealsDict,countOfFakesDict, uniqlistOfRealWords, uniqlistOfFakeWords): analyzes the words with my statistical approach.\n",
    "- programWorkStation(): calls all methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the train dataset .txt files, I made the first, reading of the file according to the selected situation so that the program can respond to the stem and stopwords options. In the current situation, operations were only performed for unigram. But this time, with using \"nltk.tokenize\" and \"nltk.stem\" libraries, stem making and token addition functions were performed for the bigram. For stemming operations, PorterStemmer() function of \"nltk.stem\" was performed very well. Adding token to the lines of train data has raised the percentage of calculation accuracy of the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test data is a .csv folder, I used the pandas library to read it in dataframe as in the previous homework. In addition, I have also opted for the ngram and stem operations. Because the test data would be compared with the model, the stem and token addition should be done for testing. First of all I have included the words in the bigram calculations by adding token to the beginning and end of each line for the Bigram option. This has raised the percentage of calculation accuracy of the process. However, token was not added for Unigram operation. Because the added token would be perceived as an extra unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemlinesOfReal_uni, stemlinesOfFake_uni = readFile(\"clean_real-Train.txt\", \"clean_fake-Train.txt\", 1, stemed=True)\n",
    "stemlinesOfReal_bi, stemlinesOfFake_bi = readFile(\"clean_real-Train.txt\", \"clean_fake-Train.txt\", 2, stemed=True)\n",
    "linesOfReal_uni, linesOfFake_uni = readFile(\"clean_real-Train.txt\", \"clean_fake-Train.txt\", 1, stemed=False)\n",
    "linesOfReal_bi, linesOfFake_bi = readFile(\"clean_real-Train.txt\", \"clean_fake-Train.txt\", 2, stemed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Understanding Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to predicting in this part, whether a headline is real or fake news from words that appear in the headline. I looked for a frequencies of words that appears in the headlines and I gave 3 examples of specific keywords that may be useful, together with statistics on how often they appear in real and fake headlines. My approach of determining word is avoiding common words in different classifiers (real or fake) so that  choosed words was special to its group with top 3 frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness count:  423\n",
      "Accuracy =  86.50306748466258\n",
      "\n",
      "Understanding Data and choosen 3 word: \n",
      "Choosen Real Words:{ korea : 62 }{ turnbull : 48 }{ travel : 47 }\n",
      "Choosen Fake Words:{ breaking : 24 }{ soros : 18 }{ woman : 13 }"
     ]
    }
   ],
   "source": [
    "returnValuesOfUnigram = bagOfWords(linesOfReal_uni, linesOfFake_uni, 1, stopEnglish=None, stemed=False)\n",
    "countOfRealsDict = returnValuesOfUnigram[0]\n",
    "countOfFakesDict = returnValuesOfUnigram[1]\n",
    "uniqlistOfRealWords = returnValuesOfUnigram[2]\n",
    "uniqlistOfFakeWords = returnValuesOfUnigram[3]\n",
    "print(\"\\nUnderstanding Data and choosen 3 word: \")\n",
    "give_three_words = understandData(countOfRealsDict, countOfFakesDict, uniqlistOfRealWords, uniqlistOfFakeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the pyplot of their counts & frequencies of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Example data\n",
    "real_words = (give_three_words[0][0], give_three_words[0][1], give_three_words[0][2])\n",
    "y_pos = np.arange(len(real_words))\n",
    "real_counts = (give_three_words[1][0], give_three_words[1][1], give_three_words[1][2])\n",
    "error = np.random.rand(len(real_words))\n",
    "\n",
    "ax.barh(y_pos, real_counts, xerr=error, align='center',\n",
    "        color='blue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(real_words)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('word frequency')\n",
    "ax.set_title('Real Words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "#print(give_three_words)\n",
    "# Example data\n",
    "fake_words = (give_three_words[2][0], give_three_words[2][1], give_three_words[2][2])\n",
    "y_pos1 = np.arange(len(fake_words))\n",
    "fake_counts = (give_three_words[3][0], give_three_words[3][1], give_three_words[3][2])\n",
    "error1 = np.random.rand(len(fake_words))\n",
    "\n",
    "ax1.barh(y_pos1, fake_counts, xerr=error1, align='center',\n",
    "        color='red', ecolor='black')\n",
    "ax1.set_yticks(y_pos1)\n",
    "ax1.set_yticklabels(fake_words)\n",
    "ax1.invert_yaxis()  # labels read top-to-bottom\n",
    "ax1.set_xlabel('word frequency')\n",
    "ax1.set_title('Fake Words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, Naive bayes used for classification problem. Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values. The problem is determining the news headlines of given test data is real or fake.  While implementing Naive Bayes, there were some desirable requirements such as implementing BoW representation, using logarithm while finding conditional probabilities and naive bayes, laplace smoothing, etc. So, I would like to mention how I met these requirements.\n",
    "\n",
    "I implemented bag of words representation with using \"CountVectorizer\" of \"sklearn\" library. Features in Bag of Words are fit, vocabulary and transform functions was used and performed very well. As known, these kind of functions are fundamentals of bag of words in machine learning. Thanks to index dictionaries of numpy arrays, I could easily find word counts and other stuff. BoW model was created for Unigram and Bigram. While creating BoW model, I used vectorizer(ngram=x,x) for splitting words into one or two pairs that was very useful for this model. After implemented Bag of Words (BoW) model, I computed the log probabilities to prevent numerical underflow when calculating multiplicative probabilities of words. \n",
    "\n",
    "Probabilities used for representation of naive Bayes Algorithm. A list of probabilities are stored in a dictionary structure for a learned naive Bayes model. This includes class probabilities and conditional probabilities. Class probabilities is the probabilities of each class in the training dataset. Conditional Probabilities is the conditional probabilities of each input value given each class value. Conditional probability formula given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w|c) = \\frac{count(w,c)+1}{count(c)+|V|+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may encounter words during classification that you havent during training. In particular, any unknown word will have probability given below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w|c) = \\frac{1}{count(c)+|V|+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I applied bayes formula with these conditional probabilites of words and prior probabilities of classes with using log10 of conditional probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a- Analyzing Effect of the Words on Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, My approach was sorting words according to their conditional probabilies and taking ten words for different scenarios such as below. if you want to see the results, you can run the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10 words with condtional probabilities whose presence most strongly predicts that the news is real\n",
    "- 10 words with condtional probabilities whose presence most strongly predicts that the news is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness count:  423\n",
      "Accuracy =  86.50306748466258\n",
      "10 words with condtional probabilities whose presence most strongly predicts that the news is real:\n",
      "1 -  {'trump': -1.1096251280195064}\n",
      "2 -  {'donald': -1.4300932329536622}\n",
      "3 -  {'trumps': -2.0141798532697237}\n",
      "4 -  {'says': -2.085451929263504}\n",
      "5 -  {'clinton': -2.4300932329536624}\n",
      "6 -  {'election': -2.436253541658481}\n",
      "7 -  {'north': -2.442502490935482}\n",
      "8 -  {'ban': -2.461807646130869}\n",
      "9 -  {'korea': -2.4820110322191558}\n",
      "10 -  {'president': -2.4889598921744835}\n",
      "\n",
      "10 words with condtional probabilities whose presence most strongly predicts that the news is fake:\n",
      "1 -  {'trump': -1.2108829021246372}\n",
      "2 -  {'donald': -1.9720894356936165}\n",
      "3 -  {'hillary': -2.1605417860941354}\n",
      "4 -  {'clinton': -2.20744065571362}\n",
      "5 -  {'just': -2.431836594343856}\n",
      "6 -  {'election': -2.4719538175518387}\n",
      "7 -  {'new': -2.508470651377601}\n",
      "8 -  {'obama': -2.548342163415293}\n",
      "9 -  {'president': -2.556775330952156}\n",
      "10 -  {'america': -2.6015876753685183}\n"
     ]
    }
   ],
   "source": [
    "returnValuesOfUnigram4 = bagOfWords(linesOfReal_uni, linesOfFake_uni, 1, stopEnglish=None, stemed=False)\n",
    "countOfRealsDict = returnValuesOfUnigram4[0]\n",
    "countOfFakesDict = returnValuesOfUnigram4[1]\n",
    "tenWordsWithCondProb(countOfRealsDict, countOfFakesDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, I suddenly realized that the **most commonly used words have to lowest priority** because of the their high frequency. And I looked for TF-IDF calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. Tf means the term frequency is the number of times the term appears in the document and Idf means the document frequency is the number of documents that contain term. Term frequency and the inverse document frequency be used for creating priority for choosing words. We can say that presence equals to (1-absence) in theory. So I take the words that have the most priorities with TF-IDF values for presence and have the lowest priorities with TF-IDF values for absence. \n",
    "- For a given tasks in assingnment, if you run the code below, you can see the result of word lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose presence most strongly predicts that the news is real.:\n",
      "1 -  no\n",
      "2 -  day\n",
      "3 -  great\n",
      "4 -  major\n",
      "5 -  head\n",
      "6 -  job\n",
      "7 -  tower\n",
      "8 -  punch\n",
      "9 -  30\n",
      "10 -  neck\n",
      "\n",
      "10 words whose absence most strongly predicts that the news is real.:\n",
      "1 -  trump\n",
      "2 -  us\n",
      "3 -  trumps\n",
      "4 -  on\n",
      "5 -  says\n",
      "6 -  donald\n",
      "7 -  with\n",
      "8 -  clinton\n",
      "9 -  election\n",
      "10 -  north\n",
      "\n",
      "10 words whose presence most strongly predicts that the news is fake.: \n",
      "1 -  twitter\n",
      "2 -  major\n",
      "3 -  great\n",
      "4 -  wants\n",
      "5 -  un\n",
      "6 -  tower\n",
      "7 -  job\n",
      "8 -  china\n",
      "9 -  30\n",
      "10 -  punch\n",
      "\n",
      "10 words whose absence most strongly predicts that the news is fake.: \n",
      "1 -  00\n",
      "2 -  money\n",
      "3 -  montel\n",
      "4 -  moo\n",
      "5 -  morale\n",
      "6 -  moralmatters\n",
      "7 -  morbidly\n",
      "8 -  mosque\n",
      "9 -  mother\n",
      "10 -  motherfucker\n"
     ]
    }
   ],
   "source": [
    "tenWordsWith_Tf_Idf(linesOfReal_uni, linesOfFake_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must to say that, TF-IDF calculations is determining the importance of the words according to their weights. But in our case, most of the words exist once in each headline. So predicting weights to the words is sometimes may be meaningless in our case. **Because, values that comes from TF-IDF operations are almost equally each other for highest priorities.** So, important words can be change randomly according to their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b- Stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of 10 non-stopwords that most strongly predict that the news is real, and the list of 10 non-stopwords that most strongly predict that the news is fake is given below. My approach of determining words is avoiding common words in different classifiers (real or fake) so that  choosed words was special to its group with top 10 conditional probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness count:  414\n",
      "Accuracy =  84.66257668711657\n",
      "10 non-stopwords that most strongly predict that the news is real:\n",
      "1 -  korea : 62\n",
      "2 -  turnbull : 48\n",
      "3 -  travel : 47\n",
      "4 -  australia : 31\n",
      "5 -  climate : 24\n",
      "6 -  paris : 20\n",
      "7 -  refugee : 19\n",
      "8 -  flynn : 16\n",
      "9 -  debate : 15\n",
      "10 -  asia : 15\n",
      "\n",
      "10 non-stopwords that most strongly predict that the news is fake:\n",
      "1 -  breaking : 24\n",
      "2 -  soros : 18\n",
      "3 -  woman : 13\n",
      "4 -  duke : 12\n",
      "5 -  steal : 11\n",
      "6 -  dr : 11\n",
      "7 -  reason : 10\n",
      "8 -  info : 9\n",
      "9 -  endingfed : 9\n",
      "10 -  interview : 9\n"
     ]
    }
   ],
   "source": [
    "returnsOfUnigram = bagOfWords(linesOfReal_uni, linesOfFake_uni, 1, stopEnglish=\"english\", stemed=False)\n",
    "countOfRealsDict = returnsOfUnigram[0]\n",
    "countOfFakesDict = returnsOfUnigram[1]\n",
    "uniqlistOfRealWords = returnsOfUnigram[2]\n",
    "uniqlistOfFakeWords = returnsOfUnigram[3]\n",
    "give_three_words = findTenNonStopWords(countOfRealsDict, countOfFakesDict, uniqlistOfRealWords, uniqlistOfFakeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c- Analyzing Effect of the Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Stopword may have the potential to increase or decrease the algorithm accuracy. In our case, I used the stopword process only in the unigram calculation. In addition, as you can see below, I have obtained the highest accuracy score without removing the stopwords and without making stem. The reason for this situation is that stopwords contribute to the news being real or fake. Because some of the stopwords are mostly used in fake headlines or real headlines, it is important to make classification. Finally, I think the stem process is also preventing the classification from being unique. That is to say, if the words in the test data are taken into calculation together with their annexes, I think the probability of matching with the classification in our model is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got the best score when I did not stopword and stem. Accuracy scores of Unigram calculations for different situations is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait till the end of program. It may take some time....\n",
      "\n",
      "Unigram (with stopWord,with stem):\n",
      "correctness count:  402\n",
      "Accuracy =  82.20858895705521\n",
      "\n",
      "Unigram (with stopWord,without stem):\n",
      "correctness count:  414\n",
      "Accuracy =  84.66257668711657\n",
      "\n",
      "Unigram (without stopWord, with stem):\n",
      "correctness count:  412\n",
      "Accuracy =  84.25357873210633\n",
      "\n",
      "Unigram (without stopWord, without stem):\n",
      "correctness count:  423\n",
      "Accuracy =  86.50306748466258\n"
     ]
    }
   ],
   "source": [
    "    print(\"Please wait till the end of program. It may take some time....\\n\")\n",
    "    print(\"Unigram (with stopWord,with stem):\")\n",
    "    rtnValuesUnigram1 = bagOfWords(stemlinesOfReal_uni, stemlinesOfFake_uni, 1, stopEnglish=\"english\", stemed=True)\n",
    "    print(\"\\nUnigram (with stopWord,without stem):\")\n",
    "    rtnValuesUnigram2 = bagOfWords(linesOfReal_uni, linesOfFake_uni, 1, stopEnglish=\"english\", stemed=False)\n",
    "    print(\"\\nUnigram (without stopWord, with stem):\")\n",
    "    rtnValuesUnigram3 = bagOfWords(stemlinesOfReal_uni, stemlinesOfFake_uni, 1, stopEnglish=None, stemed=True)\n",
    "    print(\"\\nUnigram (without stopWord, without stem):\")\n",
    "    rtnValuesUnigram4 = bagOfWords(linesOfReal_uni, linesOfFake_uni, 1, stopEnglish=None, stemed=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got the best score when I did not stopword and did stem. **Cutting of the stopwords is meaningless in Bigram but** I done for just to see the results. Accuracy scores of Bigram calculations for different situations is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start & End tokens of lines is added to the bigram calculations...\n",
      "\n",
      "Bigram (without stopwords with stem):\n",
      "correctness count:  422\n",
      "Accuracy =  86.29856850715747\n",
      "\n",
      "Bigram (without stopwords without stem):\n",
      "correctness count:  421\n",
      "Accuracy =  86.09406952965234\n",
      "\n",
      "Bigram (with stopwords with stem):\n",
      "correctness count:  400\n",
      "Accuracy =  81.79959100204499\n",
      "\n",
      "Bigram (with stopwords without stem):\n",
      "correctness count:  391\n",
      "Accuracy =  79.95910020449898\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStart & End tokens of lines is added to the bigram calculations...\")\n",
    "print(\"\\nBigram (without stopwords with stem):\")\n",
    "returnValuesOfBigram1 = bagOfWords(stemlinesOfReal_bi, stemlinesOfFake_bi, 2, stopEnglish=None, stemed=True)\n",
    "print(\"\\nBigram (without stopwords without stem):\")\n",
    "returnValuesOfBigram2 = bagOfWords(linesOfReal_bi, linesOfFake_bi, 2, stopEnglish=None, stemed=False)\n",
    "print(\"\\nBigram (with stopwords with stem):\")\n",
    "returnValuesOfBigram1 = bagOfWords(stemlinesOfReal_bi, stemlinesOfFake_bi, 2, stopEnglish=\"english\", stemed=True)\n",
    "print(\"\\nBigram (with stopwords without stem):\")\n",
    "returnValuesOfBigram2 = bagOfWords(linesOfReal_bi, linesOfFake_bi, 2, stopEnglish=\"english\", stemed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I have covered Multinomial Naive Bayes for Binary text classification. While implementing this program, I realized that there are lots of improvements in NLP and ML world that will make our life easier. I have had a lot of experience while creating my model. If I were to speak for this task, I would have been able to better train model and provide greater accuracy if the test data were larger. There were some problems I encountered. But I applied; stem, stopword, tokenizing, unigram and bigram variations and different approaches such as Bag of Words, laplace transform, TF-IDF, etc., I tried to develop classification abilities of my program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in\n",
    "- https://machinelearningmastery.com/naive-bayes-for-machine-learning/\n",
    "- http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf\n",
    "- http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/MLE.pdf\n",
    "- http://www.cs.cmu.edu/~aarti/Class/10601/homeworks/hw2Solutions.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
