{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In determining whether the news headlines are real or fake news, I will try to explain my steps in this report. In the first part, I will talk about the process of reading the train and test files and talk about the stem and tokenizing processes that I have followed to increase the accuracy calculation for the model and make the program more accurate. The second part is about understanding data topic. I will try to determine 3 examples of specific keywords that may be useful for classifiying, together with statistics on how often they appear in real and fake headlines. In the third part, I will talk about my steps and how I wrote Naive Bayes classifier implementation on Unigram and Bigram options. Unigram is the occurrences of words in a document and Bigram is the occurrences of two adjacent words in a document. Additionally, I will some hint about The bag-of-words model in here. (This is a way of representing text data when modeling text with machine learning algorithms.) In the next step, by analyzing the effects of words on predicting, I will explain the reasons by observing the change in my results by applying the stopwords process. Finally, I will give the accuracy of the calculation results and explain separately how the processes leads to a result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readFile import readFile\n",
    "from bagOfWords import bagOfWords\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from readFile import readCSV\n",
    "from naiveBayes import naiveBayes,calculationofAccuracy,understandData\n",
    "import numpy as np\n",
    "from math import log10\n",
    "from main import programWorkStation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries used for creating numpy arrays, parsing .csv files into the dataframes, parsing train .txt files, stem operations and discarding to the stopwords. Additionally, vectorizer operations was done for NLP and Machine Learning problems, such as bag of words, transform and indexing words into dictinaries. But mostly, my approaches was mostly effected the solution. i.e. for example CountVectorizer can lowercase letters, disregard punctuation and stopwords, but it can't stem. Because of that, I made stem operations with my approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**  *Read given real and fake dataset and split them according to needs of operations. (options such as stem, stopword, tokenizing, etc.)*<br>\n",
    "**--**  *After that, read given test .csv file and do the same operations on it for unigram and bigram words coupling*<br>\n",
    "**--**  *Prepare dataset for fake/real classification.*<br>\n",
    "**--**  *Filter data for different options, make stopword, stem and tokenizing on it*<br>\n",
    "**--**  *Understand the data with your own metodology.*<br>\n",
    "**--**  *Imlement Bag of Words (BoW) model.*<br>\n",
    "**--**  *Imlement Naive Bayes Algorithm for Unigram and Bigram.*<br>\n",
    "**--**  *Compute the log probabilities and make laplace smoothing on it.*<br>\n",
    "**--**  *Analyze the effect of the words on prediction. Give some approaches.*<br>\n",
    "**--**  *Calculate the accuracy of the classification which according to the given formula.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Writing Purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- readFile(trainReal, trainFake, ngram, stemed): reads trainReal and trainFake for different ngram and stemed parameters\n",
    "- readCSV(testFile, ngram, stemed): reads test.csv for different ngram and stemed parameters\n",
    "- bagOfWords(linesOfReal,linesOfFake, ngram, stopEnglish, stemed): takes option parameters, calls naive bayes and understandData functions for processes\n",
    "- naiveBayes(countOfRealsDict, BoWOfReal, countOfFakesDict, BoWOfFake, testindexDictOfWord, test_BoW): takes some count parameters and BoW of datas, after that performs naive bayes algorithm \n",
    "- calculationofAccuracy(correctnessCount, testSize): for accuracy calculation\n",
    "- understandData(countOfRealsDict,countOfFakesDict, uniqlistOfRealWords, uniqlistOfFakeWords): analyzes the words with my statistical approach.\n",
    "- programWorkStation(): calls all methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the train dataset .txt files, I made the first, reading of the file according to the selected situation so that the program can respond to the stem and stopwords options. In the current situation, operations were only performed for unigram. But this time, with using \"nltk.tokenize\" and \"nltk.stem\" libraries, stem making and token addition functions were performed for the bigram. For stemming operations, PorterStemmer() function of \"nltk.stem\" was performed very well. Adding token to the lines of train data has raised the percentage of calculation accuracy of the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test data is a .csv folder, I used the pandas library to read it in dataframe as in the previous homework. In addition, I have also opted for the ngram and stem operations. Because the test data would be compared with the model, the stem and token addition should be done for testing. First of all I have included the words in the bigram calculations by adding token to the beginning and end of each line for the Bigram option. This has raised the percentage of calculation accuracy of the process. However, token was not added for Unigram operation. Because the added token would be perceived as an extra unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "programWorkStation()\n",
    "\n",
    "tend = time.time()\n",
    "print(\"\\nRuntime: \" + str(tend-tstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Effect of the Words on Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords and Stem Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing effect of the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of my work, I can say that While focusing on the performance of the data set, I focused on enhancing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
