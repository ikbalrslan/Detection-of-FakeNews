{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In determining whether the news headlines are real or fake news, I will try to explain my steps in this report. In the first part, I will talk about the process of reading the train and test files and talk about the stem and tokenizing processes that I have followed to increase the accuracy calculation for the model and make the program more accurate. The second part is about understanding data topic. I will try to determine 3 examples of specific keywords that may be useful for classifiying, together with statistics on how often they appear in real and fake headlines. In the third part, I will talk about my steps and how I wrote Naive Bayes classifier implementation on Unigram and Bigram options. Unigram is the occurrences of words in a document and Bigram is the occurrences of two adjacent words in a document. Additionally, I will some hint about The bag-of-words model in here. (This is a way of representing text data when modeling text with machine learning algorithms.) In the next step, by analyzing the effects of words on predicting, I will explain the reasons by observing the change in my results by applying the stopwords process. Finally, I will give the accuracy of the calculation results and explain separately how the processes leads to a result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test data is a .csv folder, I used the pandas library to read it in dataframe as in the previous homework. In addition, I have also opted for the ngram and stem operations. Because the test data would be compared with the model, the stem and token addition should be done for testing. \n",
    "\n",
    "First of all I have included the words in the bigram calculations by adding token to the beginning and end of each line for the Bigram option. This has raised the percentage of calculation accuracy of the process. However, token was not added for Unigram operation. Because the added token would be perceived as an extra unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonuç cümlesi:\n",
    "\n",
    "As a result of my work, I can say that While focusing on the performance of the data set, I focused on enhancing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
